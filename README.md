
# !TSMX-ETL Documenta√ß√£o

## Vis√£o Geral

Este reposit√≥rio cont√©m o projeto **TSMX-ETL**, um pipeline de ETL (Extract, Transform, Load) desenvolvido para automatizar a importa√ß√£o, limpeza e carga de dados relacionados ao Oscar para um banco de dados PostgreSQL. O objetivo principal deste projeto √© integrar dados provenientes de arquivos em formatos `.csv`, `.xls` e `.xlsx`, process√°-los de forma eficiente, normalizar e garantir sua integridade antes de armazen√°-los em um banco de dados relacional.

Por padr√£o o pipeline utiliza o Python 3.10.11 como linguagem principal, com bibliotecas como `pandas` para processamento de dados e `psycopg2` para comunica√ß√£o com o PostgreSQL. Executado dentro de um container Docker, facilitando a portabilidade e replicabilidade do ambiente.


### üí° _Por que Python 3.10.11?_
Em alguns ambientes (especialmente Linux Debian). √â comum que o gerenciador de pacotes "for√ße" a instala√ß√£o do python 3.9.x ou 3.11.x automaticamente. Mesmo que o usu√°rio, deixe expl√≠cito que deseja uma determinada vers√£o do interpretador. Neste caso 3.10.11... 
Para fins de demonstra√ß√£o de dom√≠nio, sobre o versionamento do ambiente. Configurei especificamente, a vers√£o 3.10.11. Mas poderia ser qualquer outra espec√≠fica, a crit√©rio do dev. analista, gestor, etc...
O que pode ser observado e personalizado a seu crit√©rio/gosto no trecho: 
```bash
RUN apt-get update && apt-get install -y llvm && \
    /root/.pyenv/bin/pyenv install 3.10.11 && \
    ...
```

_Contido no arquivo "docker/config_amb". Apresentado mais a diante, em **[Estrutura do Projeto](#estrutura-do-projeto).**_
<br>&nbsp;

## Requisitos

Antes de executar o projeto, certifique-se de ter os seguintes requisitos:

### Software
- Docker e Docker-compose (obrigat√≥rios, para execu√ß√£o em containers)
- Pip gerenciador de pacotes (opcional)

### Bibliotecas Python
As bibliotecas ser√£o baixadas e incorporadas automaticamente ao cont√™iner gerado, e est√£o listadas no arquivo `requirements.txt`. Para instal√°-las no hospedeiro (caso deseje). Precisar√° possuir Pip, instalado e dispon√≠vel no host, basta executar o comando:

```bash
pip install -r requirements.txt
```

## Como usar? 

### Containers Docker
Este projeto usa o Docker para executar o banco de dados PostgreSQL, suas depend√™ncias, libs e servi√ßos relacionados. Para isso, voc√™ precisar√° de uma instala√ß√£o do Docker e Docker Compose.
Uma vez executado. Ele reunir√° as partes necess√°rias, montar√° e configurar√° um ambiente completo para execu√ß√£o deste projeto.

#### ``Em Windows:`` 
Bata executar o arquivo "BASHEXEC.CMD"
A seguir, todo o processo se dar√° automaticamente. At√© surgir um terminal consideravelmente maior, com a logo da TSMX, informando os servi√ßos ativos.
<br>&nbsp; 

#### ``Em demais plataformas.`` 
Bata executar dois comandos em seu terminal:
1¬∫ Gera ambiente docker a partir do "docker-compose.yml":
```bash
docker-compose up --build
```
Quando a montagem do ambiente j√° estiver conclu√≠da:
2¬∫ Abra o terminal do container resultante:
```bash
docker exec -it tsmx_etl bash
```
<br>&nbsp; <br>&nbsp;
### No terminal (BASH)
-----
Uma vez diante do terminal do container \tsmx_etl\ execute:
```bash
ETL
```
ou:
```bash
etl
```
igualmente aceito:
```bash
python etl.py
```

Em seguida lhe ser√° pedido o nome e local, do arquivo de origem dos dados. Recomendo alojar seus arquivos na pasta "imports/" do projeto. O que corresponder√° a "\imports\seu_arquivo.(CSV, XLS ou XLSX)" no terminal de importa√ß√£o.


## Estrutura do Projeto

A estrutura do projeto a seguir:

```
.
‚îú‚îÄ‚îÄ backups/                  # Arquivos de backup e logs
‚îú‚îÄ‚îÄ docker/                   # Arquivos de configura√ß√£o do Docker
‚îÇ   ‚îî‚îÄ‚îÄ config_amb            # Configura√ß√µes do ambiente de execu√ß√£o
‚îÇ   ‚îî‚îÄ‚îÄ schema_db.txt         # Descreve a estrutura do banco de dados
‚îÇ   ‚îî‚îÄ‚îÄ brand                 # Imagem (png) a ser exibida na apresenta√ß√£o do terminal
‚îú‚îÄ‚îÄ imports/                  # Cont√©m os arquivos de dados para importa√ß√£o
‚îú‚îÄ‚îÄ app/                      # Diret√≥rio de execu√ß√£o do processo ETL (abriga os scripts)
‚îÇ   ‚îî‚îÄ‚îÄ etl.py                # Script principal respons√°vel pela manipula√ß√£o dos dados
‚îú‚îÄ‚îÄ docker-compose.yml        # Arquivo de orquestra√ß√£o do Docker
‚îú‚îÄ‚îÄ requirements.txt          # Lista de bibliotecas do Python (consulta e insta√ß√£o opcional)
‚îî‚îÄ‚îÄ README.md                 # Este arquivo de documenta√ß√£o
```

- `etl.py`: O script principal que executa o processo ETL, incluindo leitura de arquivos, transforma√ß√£o dos dados e inser√ß√£o no banco.
- `backups/`: Diret√≥rio onde os arquivos de dados importados e logs de erro s√£o armazenados.
- `docker/`: Cont√©m os arquivos de configura√ß√£o do ambiente Docker para o interpretador python, banco de dados PostgreSQL e Adminer (interface de administra√ß√£o web).
- `imports/`: Local onde recomenda-se colocar os arquivos `.csv`, `.xls` ou `.xlsx` para serem importados.
- `docker-compose.yml`: Arquivo para orquestrar o container Docker (que abrigar√° o ambiente da aplica√ß√£o).
- `requirements.txt`: Lista das bibliotecas Python usadas no projeto.

## Fluxo do Script

### 1. Recebimento e Valida√ß√£o do Arquivo

A fun√ß√£o `receber_arquivo()` solicita ao usu√°rio o caminho de um arquivo e valida:

- Se o caminho informado existe.
- Se a extens√£o do arquivo √© suportada (`.csv`, `.xls`, `.xlsx`).

Ap√≥s a valida√ß√£o, o arquivo √© copiado para o diret√≥rio `backups/`.

### 2. Leitura dos Dados

A fun√ß√£o `carregar_arquivo()` usa a biblioteca `pandas` para importar os dados do arquivo. Caso o arquivo seja um Excel, o script detecta automaticamente o motor apropriado (`openpyxl` ou `xlrd`).

### 3. Visualiza√ß√£o Inicial

A fun√ß√£o `visualizar_arquivo()` permite ao usu√°rio visualizar as primeiras linhas do arquivo para confer√™ncia, facilitando a valida√ß√£o do conte√∫do antes do processamento completo.

### 4. Normaliza√ß√£o e limpeza dos Dados

A fun√ß√£o `normalizar_dataframe()` realiza v√°rias tarefas para garantir que os dados sejam consistentes e prontos para inser√ß√£o no banco de dados:

- Normaliza os nomes das colunas para o formato `snake_case`.
- Remove espa√ßos em branco e valores nulos.
- Converte campos booleanos (como o campo `winner`) para `True` ou `False`.
- Valida a presen√ßa de colunas obrigat√≥rias: 
  - `ceremony`
  - `year`
  - `class`
  - `category`
  - `movie`
  - `winner`

### 5. Inser√ß√£o no Banco de Dados

O script usa `psycopg2` para conectar ao banco de dados PostgreSQL. As etapas de inser√ß√£o s√£o:

- Dados de cada categoria (`tbl_oscar`, `tbl_class`, `tbl_category`, `tbl_movie`) s√£o inseridos com valida√ß√£o de duplica√ß√£o. O comando `ON CONFLICT DO NOTHING` √© utilizado para evitar inser√ß√µes duplicadas.
- Os dados normalizados dos indicados e vencedores (`tbl_nominees`) s√£o inseridos ap√≥s a valida√ß√£o.

### 6. Tratamento de Erros

Se ocorrerem erros durante o processamento de uma linha (como valores ausentes ou exce√ß√µes de tipo), essas linhas s√£o registradas e ignoradas. Os detalhes do erro s√£o registrados em um arquivo `ali√°s_de_origem.log` dentro do diret√≥rio `backups/`, contendo:

- A linha do erro
- Os dados originais da linha
- O motivo do erro
- Onde "ali√°s_de_origem" √© o nome do arquivo de origem dos dados, correspondente ao log.

Caso um arquivo de log seja gerado durante o processamento. A leitura dele ser√° oferecida de forma interativa. Podendo ser posteriormente, reexibida em tela com o comando: 
```cat(backups/ali√°s_de_origem.log)``` . 


### 7. Resumo Final

Ao final do processo de importa√ß√£o, o script exibe um resumo com:

- O n√∫mero total de registros importados com sucesso.
- O n√∫mero de registros rejeitados.
- O caminho do arquivo de log gerado (se houver rejei√ß√µes).
- Op√ß√£o (S/N) para leitura imediata do log, por meio de cat() simples e diretamente no terminal.

## Estrutura do Banco de Dados

Conforme conte√∫do SQL do arquivo "docker/schema_db.txt". O banco de dados utiliza as seguintes tabelas principais:

```sql
CREATE TABLE tbl_oscar (
    id SERIAL PRIMARY KEY,
    ceremony INTEGER UNIQUE NOT NULL,
    year INTEGER NOT NULL
);

CREATE TABLE tbl_class (
    id SERIAL PRIMARY KEY,
    description TEXT UNIQUE NOT NULL
);

CREATE TABLE tbl_category (
    id SERIAL PRIMARY KEY,
    description TEXT UNIQUE NOT NULL
);

CREATE TABLE tbl_movie (
    id SERIAL PRIMARY KEY,
    title TEXT UNIQUE NOT NULL
);

CREATE TABLE tbl_nominees (
    id SERIAL PRIMARY KEY,
    oscar_id INTEGER REFERENCES tbl_oscar(id),
    class_id INTEGER REFERENCES tbl_class(id),
    category_id INTEGER REFERENCES tbl_category(id),
    movie_id INTEGER REFERENCES tbl_movie(id),
    name TEXT,
    nominees TEXT,
    winner BOOLEAN,
    detail TEXT,
    note TEXT
);
```

### Descri√ß√£o das Tabelas:

- `tbl_oscar`: Armazena informa√ß√µes sobre a cerim√¥nia e o ano do Oscar.
- `tbl_class`: Armazena categorias de premia√ß√£o, como "Atua√ß√£o", "Produ√ß√£o", etc.
- `tbl_category`: Armazena descri√ß√µes de subcategorias de pr√™mios, como "Melhor Ator", "Melhor Filme", etc.
- `tbl_movie`: Armazena informa√ß√µes sobre os filmes indicados.
- `tbl_nominees`: Armazena os indicados, com refer√™ncias √†s outras tabelas, al√©m de informa√ß√µes como o vencedor e detalhes adicionais.

## Execu√ß√£o do Script

Para executar o processo ETL, basta rodar o script `etl.py`. Conforme j√° informado em **[No terminal (BASH)](#No-terminal-bash)**

## Auditoria e Logs

Todos os arquivos processados s√£o copiados para o diret√≥rio `backups/`. Caso ocorram erros durante o processamento, um log detalhado √© gerado contendo:

- Motivo da falha
- Linha do erro
- Dados da linha


Este log pode ser consultado logo ap√≥s o processamento ou posteriormente, acessando a pasta de backups.
veja detalhes sobre, em **[tratamento de erros](#6-tratamento-de-erros)**

## Extens√£o e Customiza√ß√£o

O c√≥digo foi desenvolvido de forma modular para facilitar a manuten√ß√£o e extens√µes:

- √â poss√≠vel adicionar novas valida√ß√µes espec√≠ficas para cada coluna.
- Novas tabelas e rela√ß√µes podem ser facilmente integradas ao sistema.
- O projeto pode ser adaptado para outros bancos de dados com m√≠nimas modifica√ß√µes.

## Sobre !TSMX-ETL

**Autor:** Victor Batista  
**GitHub:** [https://github.com/srvictorbatista](https://github.com/srvictorbatista)  
**LinkedIn:** [https://linkedin.com/in/levymac](https://linkedin.com/in/levymac)  
**Contato no Telegram:** [@LevyMac](https://t.me/levymac)    

Profissional com experi√™ncia em engenharia de dados, desenvolvimento, infraestrutura e automa√ß√µes, com foco em solu√ß√µes escal√°veis e seguras. Este projeto foi desenvolvido a partir de uma intera√ß√£o com a TSMX (Provedor de Sistemas) para avaliar minhas habilidades de automa√ß√£o em processos de dados estruturados em bancos relacionais. O que espero, ter correspondido √†s expectativas.

## Reposit√≥rio

**URL do reposit√≥rio GitHub:**  
[https://github.com/srvictorbatista/tsmx-etl](https://github.com/srvictorbatista/tsmx-etl)

O reposit√≥rio cont√©m:

- O aplica√ß√£o pipeline ETL.
- Servidor de banco de dados PostgresSQL
- Servidor web com [WEB-GUI](http://localhost:5433/) para gest√£o do banco de dados Postgres.
- O arquivo de documenta√ß√£o `README.md` (este aqui).
- Scripts auxiliares para configura√ß√£o e montagem do ambiente de execu√ß√£o.
- A estrutura de diret√≥rios, arquivos, logs e demais dados podem ser revisitados em: **[estrutura do projeto](#estrutura-do-projeto)**

**Uma vez atendendo aos requisitos deste projeto (Docker e Docker-compose). Este reposit√≥rio dever√° ser capaz de criar um ambiente completo e totalmente aut√¥nomo, para a execu√ß√£o do pipeline.**

## Objetivo do Projeto

O objetivo deste projeto √© automatizar o processo de extra√ß√£o, transforma√ß√£o e carga (ETL) de dados, provenientes de arquivos `.csv`, `.xls` e `.xlsx`, para um banco de dados PostgreSQL relacional.
Veja mais detalhes na se√ß√£o **[Sobre !TSMX-ETL](#sobre-tsmx-etl)**
<br>

## üö® Update
A adi√ß√£o do arquivo ``arquivos_complementares.zip`` contem arquivos adicionais para um proposta (teste/atualiza√ß√£o) adicional. Utilizando o mesmo ambiente.
**Composi√ß√£o do arquivo compactado:**
```
[arquivos_complementares.zip]
. 
‚îú‚îÄ‚îÄ docker/                 				# Arquivos de configura√ß√£o do Docker
‚îÇ     ‚îî‚îÄ‚îÄ schema_db_complementar.sql      		# Descreve a estrutura j√° atualizada do banco de dados
‚îú‚îÄ‚îÄ imports/                          			# Cont√©m os arquivos de dados para importa√ß√£o
‚îÇ     ‚îî‚îÄ‚îÄ datasheet_oscars_complementar.csv 	        # Dados complementares a serem vinculados a nova estrutura
‚îî‚îÄ‚îÄ app/                              			# Diret√≥rio de execu√ß√£o do processo ETL (abriga os scripts)
‚îÇ     ‚îî‚îÄ‚îÄ etl2.py                     			# Script ETL atualizado, respons√°vel pela manipula√ß√£o dos dados
‚îî‚îÄ‚îÄ Diagrama ER Complementar.png       			# Diagrama esquem√°tico ilustrando a atualiza√ß√£o proposta
    
```

## P√∫blico-Alvo

**Este projeto √© destinado a:**

- Engenheiros de dados e cientistas de dados que lidam com dados tabulares recorrentes.
- Equipes de BI/DataOps que precisam garantir a rastreabilidade e a integridade da ingest√£o de dados.
- Desenvolvedores que trabalham com pipelines de dados automatizados em ambientes corporativos.
- Gestores de infraestrutura que lidam com a configura√ß√µes de automa√ß√£o para  ambientes de desenvolvimento, seguros e modulares. 

## Tecnologias Amparadas

- **Python 3.10.+**
- **Pandas**
- **psycopg2**
- **PostgreSQL 13+**
- **openpyxl** / **xlrd** 
- **Apache:** 
- **TZ-Data**
- **PHP** / **Plugin-Adminer**

## Licen√ßa

Este projeto est√° licenciado sob a Licen√ßa MIT, no estado em que se encontra. Sem limita√ß√µes de uso ou distribui√ß√£o.

### D√∫vidas? [Fa√ßa contato com o autor, aqui](#sobre-tsmx-etl)

<br>&nbsp; <br>&nbsp; <br>&nbsp; <br>&nbsp; <br>&nbsp;
