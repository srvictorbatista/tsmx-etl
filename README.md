

# !TSMX-ETL Documenta√ß√£o

## Vis√£o Geral

Este reposit√≥rio cont√©m o projeto **TSMX-ETL**, um pipeline de ETL (Extract, Transform, Load) desenvolvido para automatizar a importa√ß√£o, limpeza e carga de dados relacionados ao Oscar para um banco de dados PostgreSQL. O objetivo principal deste projeto √© integrar dados provenientes de arquivos em formatos `.csv`, `.xls` e `.xlsx`, process√°-los de forma eficiente, normalizar e garantir sua integridade antes de armazen√°-los em um banco de dados relacional.

Por padr√£o o pipeline utiliza o Python 3.10.11 como linguagem principal, com bibliotecas como `pandas` para processamento de dados e `psycopg2` para comunica√ß√£o com o PostgreSQL. Executado dentro de um container Docker, facilitando a portabilidade e replicabilidade do ambiente.


### üí° _Por que Python 3.10.11?_
Em alguns ambientes (especialmente Linux Debian). √â comum que o gerenciador de pacotes "for√ße" a instala√ß√£o do python 3.9.x ou 3.11.x automaticamente. Mesmo que o usu√°rio, deixe expl√≠cito que deseja uma determinada vers√£o do interpretador. Neste caso 3.10.11... 
Para fins de demonstra√ß√£o de dom√≠nio, sobre o versionamento do ambiente. Configurei especificamente, a vers√£o 3.10.11. Mas poderia ser qualquer outra espec√≠fica, a crit√©rio do dev. analista, gestor, etc...
O que pode ser observado e personalizado a seu crit√©rio/gosto no trecho: 
```bash
RUN apt-get update && apt-get install -y llvm && \
    /root/.pyenv/bin/pyenv install 3.10.11 && \
    ...
```

_Contido no arquivo "docker/config_amb". Apresentado mais a diante, em **[Estrutura do Projeto](#estrutura-do-projeto).**_
<br>&nbsp;

## Requisitos

Antes de executar o projeto, certifique-se de ter os seguintes requisitos:

### Software
- Docker e Docker-compose (obrigat√≥rios, para execu√ß√£o em containers)
- Pip gerenciador de pacotes (opcional)

### Bibliotecas Python
As bibliotecas ser√£o baixadas e incorporadas automaticamente ao cont√™iner gerado, e est√£o listadas no arquivo `requirements.txt`. Para instal√°-las no hospedeiro (caso deseje). Precisar√° possuir Pip, instalado e dispon√≠vel no host, basta executar o comando:

```bash
pip install -r requirements.txt
```

## Como usar? 

### Containers Docker
Este projeto usa o Docker para executar o banco de dados PostgreSQL, suas depend√™ncias, libs e servi√ßos relacionados. Para isso, voc√™ precisar√° de uma instala√ß√£o do Docker e Docker Compose.
Uma vez executado. Ele reunir√° as partes necess√°rias, montar√° e configurar√° um ambiente completo para execu√ß√£o deste projeto.

#### ``Em Windows:`` 
Basta executar o arquivo "BASHEXEC.CMD"
A seguir, todo o processo se dar√° automaticamente. At√© surgir um terminal consideravelmente maior, com a logo da TSMX, informando os servi√ßos ativos.
<br>&nbsp; 

#### ``Em demais plataformas.`` 
Basta executar dois comandos em seu terminal:
1¬∫ Gera ambiente docker a partir do "docker-compose.yml":
```bash
docker-compose up --build
```
Quando a montagem do ambiente j√° estiver conclu√≠da:
2¬∫ Abra o terminal do container resultante:
```bash
docker exec -it tsmx_etl bash
```
<br>

### Notas Adicionais
(*) **Gestor Web Gr√°fico (WEB-GUI)**: 

Como item complementar (opcional) √© erguido automaticamente, um **servidor [WEB-GUI](http://localhost:5433/)** para gest√£o do banco de dados **Postgres** de forma gr√°fica. Igualmentre personalizado para a **TSMX**.

A interface mencionada acompanha um **lugin de auto-login** que pode ser configurado (opcionalmente) para realizar **login de forma automatica** ou realizar o preenchimento parcial das **credenciais de acesso**. 

**Senha padr√£o:** 
```
postgres
```

<br>&nbsp; <br>&nbsp;

### No terminal (BASH)
-----
Uma vez diante do terminal do container \tsmx_etl\ execute:
```bash
ETL
```
ou:
```bash
etl
```
igualmente aceito:
```bash
python etl.py
```

Em seguida lhe ser√° pedido o nome e local, do arquivo de origem dos dados. Recomendo alojar seus arquivos na pasta "imports/" do projeto. O que corresponder√° a "\imports\seu_arquivo.(CSV, XLS ou XLSX)" no terminal de importa√ß√£o.

<br> &nbsp; 

## Estrutura do Projeto

A estrutura do projeto a seguir:

```
.
‚îú‚îÄ‚îÄ backups/                  # Arquivos de backup e logs
‚îú‚îÄ‚îÄ docker/                   # Arquivos de configura√ß√£o do Docker
‚îÇ   ‚îî‚îÄ‚îÄ config_amb            # Configura√ß√µes do ambiente de execu√ß√£o
‚îÇ   ‚îî‚îÄ‚îÄ schema_db.txt         # Descreve a estrutura do banco de dados
‚îÇ   ‚îî‚îÄ‚îÄ brand                 # Imagem (png) a ser exibida na apresenta√ß√£o do terminal
‚îú‚îÄ‚îÄ imports/                  # Cont√©m os arquivos de dados para importa√ß√£o
‚îú‚îÄ‚îÄ app/                      # Diret√≥rio de execu√ß√£o do processo ETL (abriga os scripts)
‚îÇ   ‚îî‚îÄ‚îÄ etl.py                # Script principal respons√°vel pela manipula√ß√£o dos dados
‚îú‚îÄ‚îÄ docker-compose.yml        # Arquivo de orquestra√ß√£o do Docker
‚îú‚îÄ‚îÄ requirements.txt          # Lista de bibliotecas do Python (consulta e insta√ß√£o opcional)
‚îî‚îÄ‚îÄ README.md                 # Este arquivo de documenta√ß√£o
```

- `etl.py`: O script principal que executa o processo ETL, incluindo leitura de arquivos, transforma√ß√£o dos dados e inser√ß√£o no banco.
- `backups/`: Diret√≥rio onde os arquivos de dados importados e logs de erro s√£o armazenados.
- `docker/`: Cont√©m os arquivos de configura√ß√£o do ambiente Docker para o interpretador python, banco de dados PostgreSQL e Adminer (interface de administra√ß√£o web).
- `imports/`: Local onde recomenda-se colocar os arquivos `.csv`, `.xls` ou `.xlsx` para serem importados.
- `docker-compose.yml`: Arquivo para orquestrar o container Docker (que abrigar√° o ambiente da aplica√ß√£o).
- `requirements.txt`: Lista das bibliotecas Python usadas no projeto.

## Fluxo do Script

### 1. Recebimento e Valida√ß√£o do Arquivo

A fun√ß√£o `receber_arquivo()` solicita ao usu√°rio o caminho de um arquivo e valida:

- Se o caminho informado existe.
- Se a extens√£o do arquivo √© suportada (`.csv`, `.xls`, `.xlsx`).

Ap√≥s a valida√ß√£o, o arquivo √© copiado para o diret√≥rio `backups/`.

### 2. Leitura dos Dados

A fun√ß√£o `carregar_arquivo()` usa a biblioteca `pandas` para importar os dados do arquivo. Caso o arquivo seja um Excel, o script detecta automaticamente o motor apropriado (`openpyxl` ou `xlrd`).

### 3. Visualiza√ß√£o Inicial

A fun√ß√£o `visualizar_arquivo()` permite ao usu√°rio visualizar as primeiras linhas do arquivo para confer√™ncia, facilitando a valida√ß√£o do conte√∫do antes do processamento completo.


### 4. Normaliza√ß√£o e limpeza dos Dados

#### Fun√ß√£o `normalizar_dataframe`
Esta fun√ß√£o tem como objetivo **normalizar, limpar e padronizar** um `DataFrame` do `pandas`.
A fun√ß√£o `normalizar_dataframe()` realiza v√°rias tarefas para garantir que os dados sejam consistentes e prontos para inser√ß√£o no banco de dados:

```python
def normalizar_dataframe(df):
    df.columns = [col.strip().lower().replace(" ", "_") for col in df.columns]
    for col in ['note', 'detail', 'nominees', 'name', 'movie', 'category', 'class']:
        if col in df.columns:
            df[col] = df[col].fillna("").astype(str).str.strip()
    df['winner'] = df['winner'].fillna(False)
    df['winner'] = df['winner'].apply(lambda x: str(x).strip().lower() in ['true', '1', 'yes'])
    obrigatorias = ["ceremony", "year", "class", "category", "movie", "winner"]
    for col in obrigatorias:
        if col not in df.columns:
            raise Exception(f"üõë Coluna obrigat√≥ria ausente: {col}")
    return df
```
- Normaliza os nomes das colunas para o formato `snake_case`.
- Remove espa√ßos em branco e valores nulos.
- Converte campos booleanos (como o campo `winner`) para `True` ou `False`.
- Valida a presen√ßa de colunas obrigat√≥rias: 
  - `ceremony`
  - `year`
  - `class`
  - `category`
  - `movie`
  - `winner`
 
  
T√£o curta, que h√° quem passe pelo c√≥digo se perguntando **"_... mas onde est√° a limpeza e tratamento dos dados?_"** 
Por isso, n√£o se engane: _Apesar de curta. esta fun√ß√£o faz tantos tratamentos, que achei por bem. Dividir sua explica√ß√£o em **4 sub-etapas**. As quais, veremos a seguir._
 <br>&nbsp;
****************************
### Sub-etapas da Fun√ß√£o ```normalizar_dataframe```

#### 4.1. Normaliza√ß√£o dos nomes das colunas

-   Remove espa√ßos em branco no in√≠cio e fim de cada nome de coluna (`strip()`).
    
-   Converte todas as letras para min√∫sculas (`lower()`).
    
-   Substitui espa√ßos por underscores (`_`), garantindo nomes de colunas compat√≠veis com boas pr√°ticas de manipula√ß√£o de dados.
    

#### 4.2. Limpeza de colunas espec√≠ficas

```python
for col in ['note', 'detail', 'nominees', 'name', 'movie', 'category', 'class']:
    if col in df.columns:
        df[col] = df[col].fillna("").astype(str).str.strip()

```

-   Para colunas espec√≠ficas (se existirem), substitui valores nulos por strings vazias.
    
-   Garante que os dados nessas colunas sejam do tipo string.
    
-   Remove espa√ßos em branco nas extremidades dos textos. Algo similar ao ``trim()`` em outras linguagens.
    

#### 4.3. Padroniza√ß√£o da coluna `winner`

```python
df['winner'] = df['winner'].fillna(False)
df['winner'] = df['winner'].apply(lambda x: str(x).strip().lower() in ['true', '1', 'yes'])

```

-   Preenche valores nulos da coluna `winner` com `False`.
    
-   Converte os valores da coluna para string, remove espa√ßos e transforma em min√∫sculo.
    
-   Interpreta como `True` os valores que correspondem a `'true'`, `'1'` ou `'yes'`.
    
-   Todo o resto ser√° interpretado como `False`.
    

#### 4.4. Valida√ß√£o de colunas obrigat√≥rias

```python
obrigatorias = ["ceremony", "year", "class", "category", "movie", "winner"]
for col in obrigatorias:
    if col not in df.columns:
        raise Exception(f"üõë Coluna obrigat√≥ria ausente: {col}")
```

-   Define uma lista de colunas obrigat√≥rias.
    
-   Verifica se cada uma dessas colunas est√° presente no `DataFrame`.
    
-   Caso alguma coluna esteja ausente, lan√ßa uma exce√ß√£o indicando qual coluna est√° faltando.
    

----------

#### Observa√ß√µes

-   Apesar de curta. Esta fun√ß√£o √© robusta contra aus√™ncia de colunas, mas exigente com rela√ß√£o √†s colunas obrigat√≥rias. O que exigir√° aten√ß√£o extra, durante modifica√ß√µes futuras deste projeto. Dado seu poder de atua√ß√£o neste projeto.
    
-   Ela assume que a coluna `winner` pode vir com diferentes formatos booleanos (`True`, `1`, `yes`, etc.) e trata isso de forma pr√°tica.
    
-   A limpeza de texto evita problemas comuns em an√°lises de dados, como duplica√ß√µes ou agrupamentos errados causados por espa√ßos em branco ou varia√ß√µes de mai√∫sculas/min√∫sculas.

- Possui uma implementa√ß√£o extremamente curta, pr√°tica e (mod√©stia a parte), elegante. Diante de alternativas com a mesma finalidade.

- Resumidamente: √â baixinha, mas invocada! **_Por tanto, n√£o menospreze seu tamanho..._** 
Dedique a ela, o respeito e aten√ß√£o que ela merece.

****************************
 <br>&nbsp;

### 5. Inser√ß√£o no Banco de Dados

O script usa `psycopg2` para conectar ao banco de dados PostgreSQL. As etapas de inser√ß√£o s√£o:

- Dados de cada categoria (`tbl_oscar`, `tbl_class`, `tbl_category`, `tbl_movie`) s√£o inseridos com valida√ß√£o de duplica√ß√£o. O comando `ON CONFLICT DO NOTHING` √© utilizado para evitar inser√ß√µes duplicadas.
- Os dados normalizados dos indicados e vencedores (`tbl_nominees`) s√£o inseridos ap√≥s a valida√ß√£o.

### 6. Tratamento de Erros

Se ocorrerem erros durante o processamento de uma linha (como valores ausentes ou exce√ß√µes de tipo), essas linhas s√£o registradas e ignoradas. Os detalhes do erro s√£o registrados em um arquivo `ali√°s_de_origem.log` dentro do diret√≥rio `backups/`, contendo:

- A linha do erro
- Os dados originais da linha
- O motivo do erro
- Onde "ali√°s_de_origem" √© o nome do arquivo de origem dos dados, correspondente ao log.

Caso um arquivo de log seja gerado durante o processamento. A leitura dele ser√° oferecida de forma interativa. Podendo ser posteriormente, reexibida em tela com o comando: 
```cat(backups/ali√°s_de_origem.log)``` . 


### 7. Resumo Final

Ao final do processo de importa√ß√£o, o script exibe um resumo com:

- O n√∫mero total de registros importados com sucesso.
- O n√∫mero de registros rejeitados.
- O caminho do arquivo de log gerado (se houver rejei√ß√µes).
- Op√ß√£o (S/N) para leitura imediata do log, por meio de cat() simples e diretamente no terminal.

## Estrutura do Banco de Dados

Conforme conte√∫do SQL do arquivo "docker/schema_db.txt". O banco de dados utiliza as seguintes tabelas principais:

```sql
CREATE TABLE tbl_oscar (
    id SERIAL PRIMARY KEY,
    ceremony INTEGER UNIQUE NOT NULL,
    year INTEGER NOT NULL
);

CREATE TABLE tbl_class (
    id SERIAL PRIMARY KEY,
    description TEXT UNIQUE NOT NULL
);

CREATE TABLE tbl_category (
    id SERIAL PRIMARY KEY,
    description TEXT UNIQUE NOT NULL
);

CREATE TABLE tbl_movie (
    id SERIAL PRIMARY KEY,
    title TEXT UNIQUE NOT NULL
);

CREATE TABLE tbl_nominees (
    id SERIAL PRIMARY KEY,
    oscar_id INTEGER REFERENCES tbl_oscar(id),
    class_id INTEGER REFERENCES tbl_class(id),
    category_id INTEGER REFERENCES tbl_category(id),
    movie_id INTEGER REFERENCES tbl_movie(id),
    name TEXT,
    nominees TEXT,
    winner BOOLEAN,
    detail TEXT,
    note TEXT
);
```

### Descri√ß√£o das Tabelas:

- `tbl_oscar`: Armazena informa√ß√µes sobre a cerim√¥nia e o ano do Oscar.
- `tbl_class`: Armazena categorias de premia√ß√£o, como "Atua√ß√£o", "Produ√ß√£o", etc.
- `tbl_category`: Armazena descri√ß√µes de subcategorias de pr√™mios, como "Melhor Ator", "Melhor Filme", etc.
- `tbl_movie`: Armazena informa√ß√µes sobre os filmes indicados.
- `tbl_nominees`: Armazena os indicados, com refer√™ncias √†s outras tabelas, al√©m de informa√ß√µes como o vencedor e detalhes adicionais.


## Execu√ß√£o do Script

Para executar o processo ETL, basta rodar o script `etl.py`. Conforme j√° informado em **[No terminal (BASH)](#No-terminal-bash)**

## Auditoria e Logs

Todos os arquivos processados s√£o copiados para o diret√≥rio `backups/`. Caso ocorram erros durante o processamento, um log detalhado √© gerado contendo:

- Motivo da falha
- Linha do erro
- Dados da linha


Este log pode ser consultado logo ap√≥s o processamento ou posteriormente, acessando a pasta de backups.
veja detalhes sobre, em **[tratamento de erros](#6-tratamento-de-erros)**

## Extens√£o e Customiza√ß√£o

O c√≥digo foi desenvolvido de forma modular para facilitar a manuten√ß√£o e extens√µes:

- √â poss√≠vel adicionar novas valida√ß√µes espec√≠ficas para cada coluna.
- Novas tabelas e rela√ß√µes podem ser facilmente integradas ao sistema.
- O projeto pode ser adaptado para outros bancos de dados com m√≠nimas modifica√ß√µes.

## Sobre !TSMX-ETL

**Autor:** Victor Batista  
**GitHub:** [https://github.com/srvictorbatista](https://github.com/srvictorbatista)  
**LinkedIn:** [https://linkedin.com/in/levymac](https://linkedin.com/in/levymac)  
**Contato no Telegram:** [@LevyMac](https://t.me/levymac)    

Profissional com experi√™ncia em engenharia de dados, desenvolvimento, infraestrutura e automa√ß√µes, com foco em solu√ß√µes escal√°veis e seguras. Este projeto foi desenvolvido a partir de uma intera√ß√£o com a TSMX (Provedor de Sistemas) para avaliar minhas habilidades de automa√ß√£o em processos de dados estruturados em bancos relacionais. O que espero, ter correspondido √†s expectativas.

## Reposit√≥rio

**URL do reposit√≥rio GitHub:**  
[https://github.com/srvictorbatista/tsmx-etl](https://github.com/srvictorbatista/tsmx-etl)

O reposit√≥rio cont√©m:

- O aplica√ß√£o pipeline ETL.
- Servidor de banco de dados PostgresSQL
- Servidor web com [WEB-GUI](http://localhost:5433/) para gest√£o do banco de dados Postgres.
- O arquivo de documenta√ß√£o `README.md` (este aqui).
- Scripts auxiliares para configura√ß√£o e montagem do ambiente de execu√ß√£o.
- A estrutura de diret√≥rios, arquivos, logs e demais dados podem ser revisitados em: **[estrutura do projeto](#estrutura-do-projeto)**

**Uma vez atendendo aos requisitos deste projeto (Docker e Docker-compose). Este reposit√≥rio dever√° ser capaz de criar um ambiente completo e totalmente aut√¥nomo, para a execu√ß√£o do pipeline.**

## Objetivo do Projeto

O objetivo deste projeto √© automatizar o processo de extra√ß√£o, transforma√ß√£o e carga (ETL) de dados, provenientes de arquivos `.csv`, `.xls` e `.xlsx`, para um banco de dados PostgreSQL relacional.
Veja mais detalhes na se√ß√£o **[Sobre !TSMX-ETL](#sobre-tsmx-etl)**

## P√∫blico-Alvo

**Este projeto √© destinado a:**

- Engenheiros de dados e cientistas de dados que lidam com dados tabulares recorrentes.
- Equipes de BI/DataOps que precisam garantir a rastreabilidade e a integridade da ingest√£o de dados.
- Desenvolvedores que trabalham com pipelines de dados automatizados em ambientes corporativos.
- Gestores de infraestrutura que lidam com a configura√ß√µes de automa√ß√£o para  ambientes de desenvolvimento, seguros e modulares. 

## Tecnologias Amparadas

- **Python 3.10.+**
- **Pandas**
- **psycopg2**
- **PostgreSQL 13+**
- **openpyxl** / **xlrd** 
- **Apache:** 
- **TZ-Data**
- **PHP** / **Plugin-Adminer**

## Licen√ßa

Este projeto est√° licenciado sob a Licen√ßa MIT, no estado em que se encontra. Sem limita√ß√µes de uso ou distribui√ß√£o.

### D√∫vidas? [Fa√ßa contato com o autor, aqui](#sobre-tsmx-etl)

<br>&nbsp; <br>&nbsp; <br>&nbsp; <br>&nbsp; <br>&nbsp;
